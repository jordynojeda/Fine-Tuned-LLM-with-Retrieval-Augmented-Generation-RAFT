{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dbd88c8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_cpp import Llama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "909a6980",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_load_from_file_impl: using device Metal (Apple M1 Max) - 21845 MiB free\n",
      "llama_model_loader: loaded meta data with 31 key-value pairs and 292 tensors from /Users/jordynojeda/Documents/Fine-Tuned-LLM-with-Retrieval-Augmented-Generation-FT-RAG/models/Meta-Llama-3.1-8B-Instruct-bnb-4bit-financial-advisor-qlora-GGUF/financial_advisor.Q4_K_M.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.type str              = model\n",
      "llama_model_loader: - kv   2:                               general.name str              = Meta Llama 3.1 8B Instruct Bnb 4bit F...\n",
      "llama_model_loader: - kv   3:                           general.finetune str              = Instruct-bnb-4bit-financial-advisor-q...\n",
      "llama_model_loader: - kv   4:                           general.basename str              = Meta-Llama-3.1\n",
      "llama_model_loader: - kv   5:                         general.size_label str              = 8B\n",
      "llama_model_loader: - kv   6:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   7:                       llama.context_length u32              = 131072\n",
      "llama_model_loader: - kv   8:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   9:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv  10:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv  11:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv  12:                       llama.rope.freq_base f32              = 500000.000000\n",
      "llama_model_loader: - kv  13:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  14:                 llama.attention.key_length u32              = 128\n",
      "llama_model_loader: - kv  15:               llama.attention.value_length u32              = 128\n",
      "llama_model_loader: - kv  16:                           llama.vocab_size u32              = 128256\n",
      "llama_model_loader: - kv  17:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv  18:                       tokenizer.ggml.model str              = gpt2\n",
      "llama_model_loader: - kv  19:                         tokenizer.ggml.pre str              = llama-bpe\n",
      "llama_model_loader: - kv  20:                      tokenizer.ggml.tokens arr[str,128256]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
      "llama_model_loader: - kv  21:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
      "llama_model_loader: - kv  22:                      tokenizer.ggml.merges arr[str,280147]  = [\"Ġ Ġ\", \"Ġ ĠĠĠ\", \"ĠĠ ĠĠ\", \"...\n",
      "llama_model_loader: - kv  23:                tokenizer.ggml.bos_token_id u32              = 128000\n",
      "llama_model_loader: - kv  24:                tokenizer.ggml.eos_token_id u32              = 128009\n",
      "llama_model_loader: - kv  25:            tokenizer.ggml.padding_token_id u32              = 128004\n",
      "llama_model_loader: - kv  26:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  27:               tokenizer.ggml.add_sep_token bool             = false\n",
      "llama_model_loader: - kv  28:                    tokenizer.chat_template str              = \\n\\n<|start_header_id|>system<|end_head...\n",
      "llama_model_loader: - kv  29:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - kv  30:                          general.file_type u32              = 15\n",
      "llama_model_loader: - type  f32:   66 tensors\n",
      "llama_model_loader: - type q4_K:  193 tensors\n",
      "llama_model_loader: - type q6_K:   33 tensors\n",
      "print_info: file format = GGUF V3 (latest)\n",
      "print_info: file type   = Q4_K - Medium\n",
      "print_info: file size   = 4.58 GiB (4.89 BPW) \n",
      "init_tokenizer: initializing tokenizer for type 2\n",
      "load: control token: 128254 '<|reserved_special_token_246|>' is not marked as EOG\n",
      "load: control token: 128252 '<|reserved_special_token_244|>' is not marked as EOG\n",
      "load: control token: 128251 '<|reserved_special_token_243|>' is not marked as EOG\n",
      "load: control token: 128250 '<|reserved_special_token_242|>' is not marked as EOG\n",
      "load: control token: 128248 '<|reserved_special_token_240|>' is not marked as EOG\n",
      "load: control token: 128247 '<|reserved_special_token_239|>' is not marked as EOG\n",
      "load: control token: 128245 '<|reserved_special_token_237|>' is not marked as EOG\n",
      "load: control token: 128244 '<|reserved_special_token_236|>' is not marked as EOG\n",
      "load: control token: 128243 '<|reserved_special_token_235|>' is not marked as EOG\n",
      "load: control token: 128240 '<|reserved_special_token_232|>' is not marked as EOG\n",
      "load: control token: 128238 '<|reserved_special_token_230|>' is not marked as EOG\n",
      "load: control token: 128235 '<|reserved_special_token_227|>' is not marked as EOG\n",
      "load: control token: 128234 '<|reserved_special_token_226|>' is not marked as EOG\n",
      "load: control token: 128229 '<|reserved_special_token_221|>' is not marked as EOG\n",
      "load: control token: 128227 '<|reserved_special_token_219|>' is not marked as EOG\n",
      "load: control token: 128226 '<|reserved_special_token_218|>' is not marked as EOG\n",
      "load: control token: 128224 '<|reserved_special_token_216|>' is not marked as EOG\n",
      "load: control token: 128223 '<|reserved_special_token_215|>' is not marked as EOG\n",
      "load: control token: 128221 '<|reserved_special_token_213|>' is not marked as EOG\n",
      "load: control token: 128219 '<|reserved_special_token_211|>' is not marked as EOG\n",
      "load: control token: 128218 '<|reserved_special_token_210|>' is not marked as EOG\n",
      "load: control token: 128217 '<|reserved_special_token_209|>' is not marked as EOG\n",
      "load: control token: 128216 '<|reserved_special_token_208|>' is not marked as EOG\n",
      "load: control token: 128215 '<|reserved_special_token_207|>' is not marked as EOG\n",
      "load: control token: 128213 '<|reserved_special_token_205|>' is not marked as EOG\n",
      "load: control token: 128211 '<|reserved_special_token_203|>' is not marked as EOG\n",
      "load: control token: 128210 '<|reserved_special_token_202|>' is not marked as EOG\n",
      "load: control token: 128209 '<|reserved_special_token_201|>' is not marked as EOG\n",
      "load: control token: 128208 '<|reserved_special_token_200|>' is not marked as EOG\n",
      "load: control token: 128207 '<|reserved_special_token_199|>' is not marked as EOG\n",
      "load: control token: 128204 '<|reserved_special_token_196|>' is not marked as EOG\n",
      "load: control token: 128202 '<|reserved_special_token_194|>' is not marked as EOG\n",
      "load: control token: 128197 '<|reserved_special_token_189|>' is not marked as EOG\n",
      "load: control token: 128195 '<|reserved_special_token_187|>' is not marked as EOG\n",
      "load: control token: 128194 '<|reserved_special_token_186|>' is not marked as EOG\n",
      "load: control token: 128191 '<|reserved_special_token_183|>' is not marked as EOG\n",
      "load: control token: 128190 '<|reserved_special_token_182|>' is not marked as EOG\n",
      "load: control token: 128188 '<|reserved_special_token_180|>' is not marked as EOG\n",
      "load: control token: 128187 '<|reserved_special_token_179|>' is not marked as EOG\n",
      "load: control token: 128185 '<|reserved_special_token_177|>' is not marked as EOG\n",
      "load: control token: 128184 '<|reserved_special_token_176|>' is not marked as EOG\n",
      "load: control token: 128183 '<|reserved_special_token_175|>' is not marked as EOG\n",
      "load: control token: 128178 '<|reserved_special_token_170|>' is not marked as EOG\n",
      "load: control token: 128177 '<|reserved_special_token_169|>' is not marked as EOG\n",
      "load: control token: 128176 '<|reserved_special_token_168|>' is not marked as EOG\n",
      "load: control token: 128175 '<|reserved_special_token_167|>' is not marked as EOG\n",
      "load: control token: 128174 '<|reserved_special_token_166|>' is not marked as EOG\n",
      "load: control token: 128173 '<|reserved_special_token_165|>' is not marked as EOG\n",
      "load: control token: 128172 '<|reserved_special_token_164|>' is not marked as EOG\n",
      "load: control token: 128169 '<|reserved_special_token_161|>' is not marked as EOG\n",
      "load: control token: 128167 '<|reserved_special_token_159|>' is not marked as EOG\n",
      "load: control token: 128166 '<|reserved_special_token_158|>' is not marked as EOG\n",
      "load: control token: 128160 '<|reserved_special_token_152|>' is not marked as EOG\n",
      "load: control token: 128159 '<|reserved_special_token_151|>' is not marked as EOG\n",
      "load: control token: 128157 '<|reserved_special_token_149|>' is not marked as EOG\n",
      "load: control token: 128156 '<|reserved_special_token_148|>' is not marked as EOG\n",
      "load: control token: 128154 '<|reserved_special_token_146|>' is not marked as EOG\n",
      "load: control token: 128152 '<|reserved_special_token_144|>' is not marked as EOG\n",
      "load: control token: 128151 '<|reserved_special_token_143|>' is not marked as EOG\n",
      "load: control token: 128150 '<|reserved_special_token_142|>' is not marked as EOG\n",
      "load: control token: 128147 '<|reserved_special_token_139|>' is not marked as EOG\n",
      "load: control token: 128144 '<|reserved_special_token_136|>' is not marked as EOG\n",
      "load: control token: 128142 '<|reserved_special_token_134|>' is not marked as EOG\n",
      "load: control token: 128141 '<|reserved_special_token_133|>' is not marked as EOG\n",
      "load: control token: 128140 '<|reserved_special_token_132|>' is not marked as EOG\n",
      "load: control token: 128133 '<|reserved_special_token_125|>' is not marked as EOG\n",
      "load: control token: 128130 '<|reserved_special_token_122|>' is not marked as EOG\n",
      "load: control token: 128128 '<|reserved_special_token_120|>' is not marked as EOG\n",
      "load: control token: 128127 '<|reserved_special_token_119|>' is not marked as EOG\n",
      "load: control token: 128126 '<|reserved_special_token_118|>' is not marked as EOG\n",
      "load: control token: 128125 '<|reserved_special_token_117|>' is not marked as EOG\n",
      "load: control token: 128124 '<|reserved_special_token_116|>' is not marked as EOG\n",
      "load: control token: 128123 '<|reserved_special_token_115|>' is not marked as EOG\n",
      "load: control token: 128122 '<|reserved_special_token_114|>' is not marked as EOG\n",
      "load: control token: 128121 '<|reserved_special_token_113|>' is not marked as EOG\n",
      "load: control token: 128120 '<|reserved_special_token_112|>' is not marked as EOG\n",
      "load: control token: 128119 '<|reserved_special_token_111|>' is not marked as EOG\n",
      "load: control token: 128116 '<|reserved_special_token_108|>' is not marked as EOG\n",
      "load: control token: 128115 '<|reserved_special_token_107|>' is not marked as EOG\n",
      "load: control token: 128114 '<|reserved_special_token_106|>' is not marked as EOG\n",
      "load: control token: 128113 '<|reserved_special_token_105|>' is not marked as EOG\n",
      "load: control token: 128111 '<|reserved_special_token_103|>' is not marked as EOG\n",
      "load: control token: 128110 '<|reserved_special_token_102|>' is not marked as EOG\n",
      "load: control token: 128107 '<|reserved_special_token_99|>' is not marked as EOG\n",
      "load: control token: 128106 '<|reserved_special_token_98|>' is not marked as EOG\n",
      "load: control token: 128105 '<|reserved_special_token_97|>' is not marked as EOG\n",
      "load: control token: 128104 '<|reserved_special_token_96|>' is not marked as EOG\n",
      "load: control token: 128103 '<|reserved_special_token_95|>' is not marked as EOG\n",
      "load: control token: 128100 '<|reserved_special_token_92|>' is not marked as EOG\n",
      "load: control token: 128097 '<|reserved_special_token_89|>' is not marked as EOG\n",
      "load: control token: 128096 '<|reserved_special_token_88|>' is not marked as EOG\n",
      "load: control token: 128094 '<|reserved_special_token_86|>' is not marked as EOG\n",
      "load: control token: 128093 '<|reserved_special_token_85|>' is not marked as EOG\n",
      "load: control token: 128090 '<|reserved_special_token_82|>' is not marked as EOG\n",
      "load: control token: 128089 '<|reserved_special_token_81|>' is not marked as EOG\n",
      "load: control token: 128087 '<|reserved_special_token_79|>' is not marked as EOG\n",
      "load: control token: 128085 '<|reserved_special_token_77|>' is not marked as EOG\n",
      "load: control token: 128080 '<|reserved_special_token_72|>' is not marked as EOG\n",
      "load: control token: 128077 '<|reserved_special_token_69|>' is not marked as EOG\n",
      "load: control token: 128076 '<|reserved_special_token_68|>' is not marked as EOG\n",
      "load: control token: 128073 '<|reserved_special_token_65|>' is not marked as EOG\n",
      "load: control token: 128070 '<|reserved_special_token_62|>' is not marked as EOG\n",
      "load: control token: 128069 '<|reserved_special_token_61|>' is not marked as EOG\n",
      "load: control token: 128067 '<|reserved_special_token_59|>' is not marked as EOG\n",
      "load: control token: 128064 '<|reserved_special_token_56|>' is not marked as EOG\n",
      "load: control token: 128062 '<|reserved_special_token_54|>' is not marked as EOG\n",
      "load: control token: 128061 '<|reserved_special_token_53|>' is not marked as EOG\n",
      "load: control token: 128060 '<|reserved_special_token_52|>' is not marked as EOG\n",
      "load: control token: 128054 '<|reserved_special_token_46|>' is not marked as EOG\n",
      "load: control token: 128045 '<|reserved_special_token_37|>' is not marked as EOG\n",
      "load: control token: 128044 '<|reserved_special_token_36|>' is not marked as EOG\n",
      "load: control token: 128043 '<|reserved_special_token_35|>' is not marked as EOG\n",
      "load: control token: 128042 '<|reserved_special_token_34|>' is not marked as EOG\n",
      "load: control token: 128038 '<|reserved_special_token_30|>' is not marked as EOG\n",
      "load: control token: 128037 '<|reserved_special_token_29|>' is not marked as EOG\n",
      "load: control token: 128035 '<|reserved_special_token_27|>' is not marked as EOG\n",
      "load: control token: 128034 '<|reserved_special_token_26|>' is not marked as EOG\n",
      "load: control token: 128033 '<|reserved_special_token_25|>' is not marked as EOG\n",
      "load: control token: 128032 '<|reserved_special_token_24|>' is not marked as EOG\n",
      "load: control token: 128030 '<|reserved_special_token_22|>' is not marked as EOG\n",
      "load: control token: 128029 '<|reserved_special_token_21|>' is not marked as EOG\n",
      "load: control token: 128028 '<|reserved_special_token_20|>' is not marked as EOG\n",
      "load: control token: 128026 '<|reserved_special_token_18|>' is not marked as EOG\n",
      "load: control token: 128025 '<|reserved_special_token_17|>' is not marked as EOG\n",
      "load: control token: 128024 '<|reserved_special_token_16|>' is not marked as EOG\n",
      "load: control token: 128022 '<|reserved_special_token_14|>' is not marked as EOG\n",
      "load: control token: 128020 '<|reserved_special_token_12|>' is not marked as EOG\n",
      "load: control token: 128017 '<|reserved_special_token_9|>' is not marked as EOG\n",
      "load: control token: 128016 '<|reserved_special_token_8|>' is not marked as EOG\n",
      "load: control token: 128015 '<|reserved_special_token_7|>' is not marked as EOG\n",
      "load: control token: 128014 '<|reserved_special_token_6|>' is not marked as EOG\n",
      "load: control token: 128013 '<|reserved_special_token_5|>' is not marked as EOG\n",
      "load: control token: 128011 '<|reserved_special_token_3|>' is not marked as EOG\n",
      "load: control token: 128010 '<|python_tag|>' is not marked as EOG\n",
      "load: control token: 128006 '<|start_header_id|>' is not marked as EOG\n",
      "load: control token: 128003 '<|reserved_special_token_1|>' is not marked as EOG\n",
      "load: control token: 128002 '<|reserved_special_token_0|>' is not marked as EOG\n",
      "load: control token: 128000 '<|begin_of_text|>' is not marked as EOG\n",
      "load: control token: 128041 '<|reserved_special_token_33|>' is not marked as EOG\n",
      "load: control token: 128063 '<|reserved_special_token_55|>' is not marked as EOG\n",
      "load: control token: 128046 '<|reserved_special_token_38|>' is not marked as EOG\n",
      "load: control token: 128007 '<|end_header_id|>' is not marked as EOG\n",
      "load: control token: 128065 '<|reserved_special_token_57|>' is not marked as EOG\n",
      "load: control token: 128171 '<|reserved_special_token_163|>' is not marked as EOG\n",
      "load: control token: 128162 '<|reserved_special_token_154|>' is not marked as EOG\n",
      "load: control token: 128165 '<|reserved_special_token_157|>' is not marked as EOG\n",
      "load: control token: 128057 '<|reserved_special_token_49|>' is not marked as EOG\n",
      "load: control token: 128050 '<|reserved_special_token_42|>' is not marked as EOG\n",
      "load: control token: 128056 '<|reserved_special_token_48|>' is not marked as EOG\n",
      "load: control token: 128230 '<|reserved_special_token_222|>' is not marked as EOG\n",
      "load: control token: 128098 '<|reserved_special_token_90|>' is not marked as EOG\n",
      "load: control token: 128153 '<|reserved_special_token_145|>' is not marked as EOG\n",
      "load: control token: 128084 '<|reserved_special_token_76|>' is not marked as EOG\n",
      "load: control token: 128082 '<|reserved_special_token_74|>' is not marked as EOG\n",
      "load: control token: 128102 '<|reserved_special_token_94|>' is not marked as EOG\n",
      "load: control token: 128253 '<|reserved_special_token_245|>' is not marked as EOG\n",
      "load: control token: 128179 '<|reserved_special_token_171|>' is not marked as EOG\n",
      "load: control token: 128071 '<|reserved_special_token_63|>' is not marked as EOG\n",
      "load: control token: 128135 '<|reserved_special_token_127|>' is not marked as EOG\n",
      "load: control token: 128161 '<|reserved_special_token_153|>' is not marked as EOG\n",
      "load: control token: 128164 '<|reserved_special_token_156|>' is not marked as EOG\n",
      "load: control token: 128134 '<|reserved_special_token_126|>' is not marked as EOG\n",
      "load: control token: 128249 '<|reserved_special_token_241|>' is not marked as EOG\n",
      "load: control token: 128004 '<|finetune_right_pad_id|>' is not marked as EOG\n",
      "load: control token: 128036 '<|reserved_special_token_28|>' is not marked as EOG\n",
      "load: control token: 128148 '<|reserved_special_token_140|>' is not marked as EOG\n",
      "load: control token: 128181 '<|reserved_special_token_173|>' is not marked as EOG\n",
      "load: control token: 128222 '<|reserved_special_token_214|>' is not marked as EOG\n",
      "load: control token: 128075 '<|reserved_special_token_67|>' is not marked as EOG\n",
      "load: control token: 128241 '<|reserved_special_token_233|>' is not marked as EOG\n",
      "load: control token: 128051 '<|reserved_special_token_43|>' is not marked as EOG\n",
      "load: control token: 128068 '<|reserved_special_token_60|>' is not marked as EOG\n",
      "load: control token: 128149 '<|reserved_special_token_141|>' is not marked as EOG\n",
      "load: control token: 128201 '<|reserved_special_token_193|>' is not marked as EOG\n",
      "load: control token: 128058 '<|reserved_special_token_50|>' is not marked as EOG\n",
      "load: control token: 128146 '<|reserved_special_token_138|>' is not marked as EOG\n",
      "load: control token: 128143 '<|reserved_special_token_135|>' is not marked as EOG\n",
      "load: control token: 128023 '<|reserved_special_token_15|>' is not marked as EOG\n",
      "load: control token: 128039 '<|reserved_special_token_31|>' is not marked as EOG\n",
      "load: control token: 128132 '<|reserved_special_token_124|>' is not marked as EOG\n",
      "load: control token: 128101 '<|reserved_special_token_93|>' is not marked as EOG\n",
      "load: control token: 128212 '<|reserved_special_token_204|>' is not marked as EOG\n",
      "load: control token: 128189 '<|reserved_special_token_181|>' is not marked as EOG\n",
      "load: control token: 128225 '<|reserved_special_token_217|>' is not marked as EOG\n",
      "load: control token: 128129 '<|reserved_special_token_121|>' is not marked as EOG\n",
      "load: control token: 128005 '<|reserved_special_token_2|>' is not marked as EOG\n",
      "load: control token: 128078 '<|reserved_special_token_70|>' is not marked as EOG\n",
      "load: control token: 128163 '<|reserved_special_token_155|>' is not marked as EOG\n",
      "load: control token: 128072 '<|reserved_special_token_64|>' is not marked as EOG\n",
      "load: control token: 128112 '<|reserved_special_token_104|>' is not marked as EOG\n",
      "load: control token: 128186 '<|reserved_special_token_178|>' is not marked as EOG\n",
      "load: control token: 128095 '<|reserved_special_token_87|>' is not marked as EOG\n",
      "load: control token: 128109 '<|reserved_special_token_101|>' is not marked as EOG\n",
      "load: control token: 128099 '<|reserved_special_token_91|>' is not marked as EOG\n",
      "load: control token: 128138 '<|reserved_special_token_130|>' is not marked as EOG\n",
      "load: control token: 128193 '<|reserved_special_token_185|>' is not marked as EOG\n",
      "load: control token: 128199 '<|reserved_special_token_191|>' is not marked as EOG\n",
      "load: control token: 128048 '<|reserved_special_token_40|>' is not marked as EOG\n",
      "load: control token: 128088 '<|reserved_special_token_80|>' is not marked as EOG\n",
      "load: control token: 128192 '<|reserved_special_token_184|>' is not marked as EOG\n",
      "load: control token: 128136 '<|reserved_special_token_128|>' is not marked as EOG\n",
      "load: control token: 128092 '<|reserved_special_token_84|>' is not marked as EOG\n",
      "load: control token: 128158 '<|reserved_special_token_150|>' is not marked as EOG\n",
      "load: control token: 128001 '<|end_of_text|>' is not marked as EOG\n",
      "load: control token: 128049 '<|reserved_special_token_41|>' is not marked as EOG\n",
      "load: control token: 128031 '<|reserved_special_token_23|>' is not marked as EOG\n",
      "load: control token: 128255 '<|reserved_special_token_247|>' is not marked as EOG\n",
      "load: control token: 128182 '<|reserved_special_token_174|>' is not marked as EOG\n",
      "load: control token: 128066 '<|reserved_special_token_58|>' is not marked as EOG\n",
      "load: control token: 128180 '<|reserved_special_token_172|>' is not marked as EOG\n",
      "load: control token: 128233 '<|reserved_special_token_225|>' is not marked as EOG\n",
      "load: control token: 128079 '<|reserved_special_token_71|>' is not marked as EOG\n",
      "load: control token: 128081 '<|reserved_special_token_73|>' is not marked as EOG\n",
      "load: control token: 128231 '<|reserved_special_token_223|>' is not marked as EOG\n",
      "load: control token: 128196 '<|reserved_special_token_188|>' is not marked as EOG\n",
      "load: control token: 128047 '<|reserved_special_token_39|>' is not marked as EOG\n",
      "load: control token: 128083 '<|reserved_special_token_75|>' is not marked as EOG\n",
      "load: control token: 128139 '<|reserved_special_token_131|>' is not marked as EOG\n",
      "load: control token: 128131 '<|reserved_special_token_123|>' is not marked as EOG\n",
      "load: control token: 128118 '<|reserved_special_token_110|>' is not marked as EOG\n",
      "load: control token: 128053 '<|reserved_special_token_45|>' is not marked as EOG\n",
      "load: control token: 128220 '<|reserved_special_token_212|>' is not marked as EOG\n",
      "load: control token: 128108 '<|reserved_special_token_100|>' is not marked as EOG\n",
      "load: control token: 128091 '<|reserved_special_token_83|>' is not marked as EOG\n",
      "load: control token: 128203 '<|reserved_special_token_195|>' is not marked as EOG\n",
      "load: control token: 128059 '<|reserved_special_token_51|>' is not marked as EOG\n",
      "load: control token: 128019 '<|reserved_special_token_11|>' is not marked as EOG\n",
      "load: control token: 128170 '<|reserved_special_token_162|>' is not marked as EOG\n",
      "load: control token: 128205 '<|reserved_special_token_197|>' is not marked as EOG\n",
      "load: control token: 128040 '<|reserved_special_token_32|>' is not marked as EOG\n",
      "load: control token: 128200 '<|reserved_special_token_192|>' is not marked as EOG\n",
      "load: control token: 128236 '<|reserved_special_token_228|>' is not marked as EOG\n",
      "load: control token: 128145 '<|reserved_special_token_137|>' is not marked as EOG\n",
      "load: control token: 128168 '<|reserved_special_token_160|>' is not marked as EOG\n",
      "load: control token: 128214 '<|reserved_special_token_206|>' is not marked as EOG\n",
      "load: control token: 128137 '<|reserved_special_token_129|>' is not marked as EOG\n",
      "load: control token: 128232 '<|reserved_special_token_224|>' is not marked as EOG\n",
      "load: control token: 128239 '<|reserved_special_token_231|>' is not marked as EOG\n",
      "load: control token: 128055 '<|reserved_special_token_47|>' is not marked as EOG\n",
      "load: control token: 128228 '<|reserved_special_token_220|>' is not marked as EOG\n",
      "load: control token: 128206 '<|reserved_special_token_198|>' is not marked as EOG\n",
      "load: control token: 128018 '<|reserved_special_token_10|>' is not marked as EOG\n",
      "load: control token: 128012 '<|reserved_special_token_4|>' is not marked as EOG\n",
      "load: control token: 128198 '<|reserved_special_token_190|>' is not marked as EOG\n",
      "load: control token: 128021 '<|reserved_special_token_13|>' is not marked as EOG\n",
      "load: control token: 128086 '<|reserved_special_token_78|>' is not marked as EOG\n",
      "load: control token: 128074 '<|reserved_special_token_66|>' is not marked as EOG\n",
      "load: control token: 128027 '<|reserved_special_token_19|>' is not marked as EOG\n",
      "load: control token: 128242 '<|reserved_special_token_234|>' is not marked as EOG\n",
      "load: control token: 128155 '<|reserved_special_token_147|>' is not marked as EOG\n",
      "load: control token: 128052 '<|reserved_special_token_44|>' is not marked as EOG\n",
      "load: control token: 128246 '<|reserved_special_token_238|>' is not marked as EOG\n",
      "load: control token: 128117 '<|reserved_special_token_109|>' is not marked as EOG\n",
      "load: control token: 128237 '<|reserved_special_token_229|>' is not marked as EOG\n",
      "load: special tokens cache size = 256\n",
      "load: token to piece cache size = 0.7999 MB\n",
      "print_info: arch             = llama\n",
      "print_info: vocab_only       = 0\n",
      "print_info: n_ctx_train      = 131072\n",
      "print_info: n_embd           = 4096\n",
      "print_info: n_layer          = 32\n",
      "print_info: n_head           = 32\n",
      "print_info: n_head_kv        = 8\n",
      "print_info: n_rot            = 128\n",
      "print_info: n_swa            = 0\n",
      "print_info: n_swa_pattern    = 1\n",
      "print_info: n_embd_head_k    = 128\n",
      "print_info: n_embd_head_v    = 128\n",
      "print_info: n_gqa            = 4\n",
      "print_info: n_embd_k_gqa     = 1024\n",
      "print_info: n_embd_v_gqa     = 1024\n",
      "print_info: f_norm_eps       = 0.0e+00\n",
      "print_info: f_norm_rms_eps   = 1.0e-05\n",
      "print_info: f_clamp_kqv      = 0.0e+00\n",
      "print_info: f_max_alibi_bias = 0.0e+00\n",
      "print_info: f_logit_scale    = 0.0e+00\n",
      "print_info: f_attn_scale     = 0.0e+00\n",
      "print_info: n_ff             = 14336\n",
      "print_info: n_expert         = 0\n",
      "print_info: n_expert_used    = 0\n",
      "print_info: causal attn      = 1\n",
      "print_info: pooling type     = 0\n",
      "print_info: rope type        = 0\n",
      "print_info: rope scaling     = linear\n",
      "print_info: freq_base_train  = 500000.0\n",
      "print_info: freq_scale_train = 1\n",
      "print_info: n_ctx_orig_yarn  = 131072\n",
      "print_info: rope_finetuned   = unknown\n",
      "print_info: ssm_d_conv       = 0\n",
      "print_info: ssm_d_inner      = 0\n",
      "print_info: ssm_d_state      = 0\n",
      "print_info: ssm_dt_rank      = 0\n",
      "print_info: ssm_dt_b_c_rms   = 0\n",
      "print_info: model type       = 8B\n",
      "print_info: model params     = 8.03 B\n",
      "print_info: general.name     = Meta Llama 3.1 8B Instruct Bnb 4bit Financial Advisor Qlora GGUF\n",
      "print_info: vocab type       = BPE\n",
      "print_info: n_vocab          = 128256\n",
      "print_info: n_merges         = 280147\n",
      "print_info: BOS token        = 128000 '<|begin_of_text|>'\n",
      "print_info: EOS token        = 128009 '<|eot_id|>'\n",
      "print_info: EOT token        = 128009 '<|eot_id|>'\n",
      "print_info: EOM token        = 128008 '<|eom_id|>'\n",
      "print_info: PAD token        = 128004 '<|finetune_right_pad_id|>'\n",
      "print_info: LF token         = 198 'Ċ'\n",
      "print_info: EOG token        = 128008 '<|eom_id|>'\n",
      "print_info: EOG token        = 128009 '<|eot_id|>'\n",
      "print_info: max token length = 256\n",
      "load_tensors: loading model tensors, this can take a while... (mmap = true)\n",
      "load_tensors: layer   0 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   1 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   2 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   3 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   4 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   5 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   6 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   7 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   8 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   9 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  10 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  11 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  12 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  13 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  14 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  15 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  16 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  17 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  18 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  19 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  20 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  21 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  22 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  23 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  24 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  25 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  26 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  27 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  28 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  29 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  30 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  31 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  32 assigned to device CPU, is_swa = 0\n",
      "load_tensors: tensor 'token_embd.weight' (q4_K) (and 322 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead\n",
      "load_tensors: offloading 0 repeating layers to GPU\n",
      "load_tensors: offloaded 0/33 layers to GPU\n",
      "load_tensors:   CPU_Mapped model buffer size =  4685.30 MiB\n",
      "........................................................................................\n",
      "llama_context: constructing llama_context\n",
      "llama_context: n_seq_max     = 1\n",
      "llama_context: n_ctx         = 512\n",
      "llama_context: n_ctx_per_seq = 512\n",
      "llama_context: n_batch       = 512\n",
      "llama_context: n_ubatch      = 512\n",
      "llama_context: causal_attn   = 1\n",
      "llama_context: flash_attn    = 0\n",
      "llama_context: freq_base     = 500000.0\n",
      "llama_context: freq_scale    = 1\n",
      "llama_context: n_ctx_per_seq (512) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M1 Max\n",
      "ggml_metal_init: picking default device: Apple M1 Max\n",
      "ggml_metal_load_library: using embedded metal library\n",
      "ggml_metal_init: GPU name:   Apple M1 Max\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple7  (1007)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction   = true\n",
      "ggml_metal_init: simdgroup matrix mul. = true\n",
      "ggml_metal_init: has residency sets    = true\n",
      "ggml_metal_init: has bfloat            = true\n",
      "ggml_metal_init: use bfloat            = false\n",
      "ggml_metal_init: hasUnifiedMemory      = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 22906.50 MB\n",
      "ggml_metal_init: loaded kernel_add                                    0x103cd5850 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_add_row                                0x12b404110 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_sub                                    0x1026e25d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_sub_row                                0x1026e39d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul                                    0x1026e4340 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_row                                0x103aee7f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_div                                    0x103aeed60 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_div_row                                0x1026e4ee0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_repeat_f32                             0x1026e2c70 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_repeat_f16                             0x103aedf60 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_repeat_i32                             0x153252760 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_repeat_i16                             0x153251e10 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_scale                                  0x103af0bd0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_scale_4                                0x10462b7b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_clamp                                  0x12b3b2ec0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_tanh                                   0x10462c160 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_relu                                   0x10462cd20 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_sigmoid                                0x103af2220 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_gelu                                   0x10462d860 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_gelu_4                                 0x12b3b2a70 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_gelu_quick                             0x153034420 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_gelu_quick_4                           0x103af27b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_silu                                   0x12b3b4a10 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_silu_4                                 0x153253650 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_elu                                    0x12b3b45f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_soft_max_f16                           0x153254ad0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_soft_max_f16_4                         0x103af2d10 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_soft_max_f32                           0x103af39f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_soft_max_f32_4                         0x10462dde0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_diag_mask_inf                          0x10462e760 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x103af4830 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_f32                           0x103cdbf50 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_f16                           0x103cdc850 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_0                          0x103af5140 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_1                          0x10462f150 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q5_0                          0x103af43d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q5_1                          0x10462fce0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q8_0                          0x104687140 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q2_K                          0x104687ad0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q3_K                          0x10462e280 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_K                          0x104688970 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q5_K                          0x103af6200 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q6_K                          0x103af6fb0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x1026e5f60 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x153253fe0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x12b3b27e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x12b3b66f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x103cdd0e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x103af7c00 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x1026e5950 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x103af8850 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x103af91a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_i32                           0x12b3b6f60 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rms_norm                               0x104689270 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_l2_norm                                0x104689b70 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_group_norm                             0x10468a500 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_norm                                   0x10468ae30 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_ssm_conv_f32                           0x104688440 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_ssm_scan_f32                           0x12b3b7890 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rwkv_wkv6_f32                          0x1026e6ea0 | th_max =  384 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rwkv_wkv7_f32                          0x103cdd4f0 | th_max =  448 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x153252070 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)\n",
      "ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x1532522d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x153254400 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x103af98b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x1026e6a20 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x103afa240 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x103afab90 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x103afb490 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x1026e8490 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x1026e8de0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x153255050 | th_max =  896 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x103afba80 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x103cdf820 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x10468bcb0 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x103ce01d0 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x103cdf330 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x10468c670 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x103afc3f0 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x103afcd80 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x103ce0a90 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x10468d000 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x103afd610 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x103ce12c0 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x103ce1710 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x103ce22d0 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x1026e9410 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x1026ea030 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x103ce2e00 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x1026e9b00 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x1026ea970 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x10468dcf0 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x1026eb030 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x1026eb6a0 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x103ce3470 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x1026ec330 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x103affae0 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x12ca04500 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x1026ecd10 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x12ca04eb0 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x103ce37b0 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x103ce3df0 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x1026ebab0 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x1026ebdc0 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x103aff560 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x1026ed6d0 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x103ce4460 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x1026edf80 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x10468e500 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x12ca06b90 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x12ca063f0 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x12ca06070 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x12ca08470 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x10468fce0 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x1026ee6c0 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x1532565a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x10468e9f0 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x12ca08d80 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x1046912c0 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x1026eec40 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x12b3b5c90 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x12b404aa0 | th_max =  448 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x12b3b8350 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x12b3b8a30 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x153257020 | th_max =  896 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x153256c50 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x12b3b8c90 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x1046908b0 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x1046922f0 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x1026ef870 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x1026f0c60 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x103ce4c00 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x104691dc0 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x103ce57d0 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x104693620 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x12ca09670 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x12ca0ab20 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x103ce6090 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x12ca0a110 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x1026f0520 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x104693020 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x12ca0be90 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x104694940 | th_max =  448 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x1026f2000 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x12ca0b5d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x12ca0c910 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x1046940a0 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x104695c40 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)\n",
      "ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x12ca0d2b0 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x1026f1720 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x1026f3320 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x1026f3c60 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x104696580 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x1026f4570 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x1026f4eb0 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x1026f57d0 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x1026f6140 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x12ca0da70 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x12b3b9140 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x12b3ba140 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x104697180 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x104697bb0 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x104698250 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x1046984b0 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x104699200 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x104699b70 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x1026f6d50 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x12ca0edd0 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x12ca0f780 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_mul_mm_id_bf16_f32                (not supported)\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x103ce71b0 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x10469a8a0 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x12ca0e8b0 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x1026f7410 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x12ca10b60 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x1026f7d90 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x1026f8740 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x153033de0 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x10469b580 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x103ce7d80 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x1026f8e10 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x153255f40 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x12ca11440 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x10469b0a0 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x10469bf60 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x12b3bb150 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x10469c9a0 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x10469d350 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x10469e630 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rope_norm_f32                          0x103ce8740 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rope_norm_f16                          0x153033340 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rope_neox_f32                          0x12b3bbb20 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rope_neox_f16                          0x153258360 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_im2col_f16                             0x1026f9a30 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_im2col_f32                             0x10469eff0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_im2col_ext_f16                         0x1026f92b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_im2col_ext_f32                         0x1026fa900 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x1026fb260 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x1026fbc00 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_upscale_f32                            0x10469fc60 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_pad_f32                                0x1046a0560 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x1046a0ec0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x1046a1820 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_arange_f32                             0x12b3baa70 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x1530335a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x12b3bacd0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_leaky_relu_f32                         0x12b3bd320 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x12b3bca90 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x103ce8de0 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x103ce9450 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x103ce9ac0 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x1046a2860 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_f16_h192                0x12ca12660 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_f16_hk192_hv128         0x12ca128c0 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x12ca131e0 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_f16_hk576_hv512         0x1046a3810 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x12ca13b40 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x12ca13da0 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x1046a41e0 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x1046a3210 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x1046a4bc0 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h192               0x1026fa480 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_hk192_hv128        0x103cea160 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x1046a4f60 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_hk576_hv512        0x12ca14220 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x1046a5c20 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x12b404e00 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x103ceae00 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x1532585c0 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x103cea900 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h192               0x12b3bdb80 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_hk192_hv128        0x103ceb8a0 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x103cebf40 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_hk576_hv512        0x103cec5b0 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x103cecc20 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x12b3be5d0 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x103ced260 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x103ced900 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x103cedfa0 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h192               0x12ca15c40 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_hk192_hv128        0x1026c6f30 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x103cee610 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_hk576_hv512        0x103ceec80 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x1026c7e20 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x1026fcf90 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x103cef320 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x1026fd610 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x103cef990 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h192               0x1026fdc90 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_hk192_hv128        0x103cf0000 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x103cf06a0 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_hk576_hv512        0x103cf0d30 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x1026fe990 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x103cf13d0 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x1046a8110 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x1046a8780 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x103cf17d0 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h192               0x12ca17480 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_hk192_hv128        0x1026fed30 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x1046a89e0 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_hk576_hv512        0x12b3beb60 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h96             0x153258db0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h96            0x1046a8fa0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h96            0x1046ab6b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h96            0x12ca17b10 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h96            0x12ca17e00 | th_max =  896 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h96            0x1026ffc50 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x1046aabd0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x153032fc0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x1530356f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x1046aca20 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x1046aa4d0 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x103cf2700 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h192            0x1046ad300 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h192           0x103cf3110 | th_max =  896 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h192           0x103cf1d70 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h192           0x103cf3be0 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h192           0x12ca18c70 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h192           0x12ca19720 | th_max =  896 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_hk192_hv128      0x12ca1a910 | th_max =  896 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_hk192_hv128      0x106e04440 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_hk192_hv128      0x103cf5250 | th_max =  896 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_hk192_hv128      0x103cf5c60 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_hk192_hv128      0x106e046a0 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_hk192_hv128      0x12ca1ab70 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x12ca1bb20 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x1046adee0 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x1046ae9d0 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x12ca1c350 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x103cf6610 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x1046af390 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_hk576_hv512      0x103cf4c80 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_hk576_hv512      0x12ca1c660 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_hk576_hv512      0x12ca1d390 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_hk576_hv512      0x12ca1db00 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_hk576_hv512      0x12b3bf3c0 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_hk576_hv512      0x12ca1dd60 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_set_f32                                0x12ca1f930 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_set_i32                                0x106e060c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f32                            0x106e06a10 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f16                            0x106e07350 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)\n",
      "ggml_metal_init: loaded kernel_cpy_f16_f32                            0x106e07c70 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f16_f16                            0x1046b0810 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)\n",
      "ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x12ca20290 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x106e08570 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x1046aff10 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x106e08e90 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x106e05b00 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x12ca1f030 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_q4_0_f32                           0x153259580 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_q4_0_f16                           0x1530363b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_q4_1_f32                           0x106e09db0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_q4_1_f16                           0x106e0a710 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_q5_0_f32                           0x106e0afe0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_q5_0_f16                           0x15325aaa0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_q5_1_f32                           0x12b405060 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_q5_1_f16                           0x106e099a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_q8_0_f32                           0x1046b1920 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_q8_0_f16                           0x12b4062d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_concat                                 0x106e0cea0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_sqr                                    0x103cf7870 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_sqrt                                   0x12ca21710 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_sin                                    0x1046b28b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cos                                    0x106e0d9a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_neg                                    0x12ca222b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_sum_rows                               0x1046b31c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_argmax                                 0x1046b3b10 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x1046b4480 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x1046b4df0 | th_max = 1024 | th_width =   32\n",
      "set_abort_callback: call\n",
      "llama_context:        CPU  output buffer size =     0.49 MiB\n",
      "create_memory: n_ctx = 512 (padded)\n",
      "llama_kv_cache_unified: kv_size = 512, type_k = 'f16', type_v = 'f16', n_layer = 32, can_shift = 1, padding = 32\n",
      "llama_kv_cache_unified: layer   0: dev = CPU\n",
      "llama_kv_cache_unified: layer   1: dev = CPU\n",
      "llama_kv_cache_unified: layer   2: dev = CPU\n",
      "llama_kv_cache_unified: layer   3: dev = CPU\n",
      "llama_kv_cache_unified: layer   4: dev = CPU\n",
      "llama_kv_cache_unified: layer   5: dev = CPU\n",
      "llama_kv_cache_unified: layer   6: dev = CPU\n",
      "llama_kv_cache_unified: layer   7: dev = CPU\n",
      "llama_kv_cache_unified: layer   8: dev = CPU\n",
      "llama_kv_cache_unified: layer   9: dev = CPU\n",
      "llama_kv_cache_unified: layer  10: dev = CPU\n",
      "llama_kv_cache_unified: layer  11: dev = CPU\n",
      "llama_kv_cache_unified: layer  12: dev = CPU\n",
      "llama_kv_cache_unified: layer  13: dev = CPU\n",
      "llama_kv_cache_unified: layer  14: dev = CPU\n",
      "llama_kv_cache_unified: layer  15: dev = CPU\n",
      "llama_kv_cache_unified: layer  16: dev = CPU\n",
      "llama_kv_cache_unified: layer  17: dev = CPU\n",
      "llama_kv_cache_unified: layer  18: dev = CPU\n",
      "llama_kv_cache_unified: layer  19: dev = CPU\n",
      "llama_kv_cache_unified: layer  20: dev = CPU\n",
      "llama_kv_cache_unified: layer  21: dev = CPU\n",
      "llama_kv_cache_unified: layer  22: dev = CPU\n",
      "llama_kv_cache_unified: layer  23: dev = CPU\n",
      "llama_kv_cache_unified: layer  24: dev = CPU\n",
      "llama_kv_cache_unified: layer  25: dev = CPU\n",
      "llama_kv_cache_unified: layer  26: dev = CPU\n",
      "llama_kv_cache_unified: layer  27: dev = CPU\n",
      "llama_kv_cache_unified: layer  28: dev = CPU\n",
      "llama_kv_cache_unified: layer  29: dev = CPU\n",
      "llama_kv_cache_unified: layer  30: dev = CPU\n",
      "llama_kv_cache_unified: layer  31: dev = CPU\n",
      "llama_kv_cache_unified:        CPU KV buffer size =    64.00 MiB\n",
      "llama_kv_cache_unified: KV self size  =   64.00 MiB, K (f16):   32.00 MiB, V (f16):   32.00 MiB\n",
      "llama_context: enumerating backends\n",
      "llama_context: backend_ptrs.size() = 3\n",
      "llama_context: max_nodes = 65536\n",
      "llama_context: worst-case: n_tokens = 512, n_seqs = 1, n_outputs = 0\n",
      "llama_context: reserving graph for n_tokens = 512, n_seqs = 1\n",
      "llama_context: reserving graph for n_tokens = 1, n_seqs = 1\n",
      "llama_context: reserving graph for n_tokens = 512, n_seqs = 1\n",
      "llama_context:        CPU compute buffer size =   258.50 MiB\n",
      "llama_context: graph nodes  = 1094\n",
      "llama_context: graph splits = 514 (with bs=512), 1 (with bs=1)\n",
      "Metal : EMBED_LIBRARY = 1 | CPU : ARM_FMA = 1 | FP16_VA = 1 | DOTPROD = 1 | LLAMAFILE = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | \n",
      "Model metadata: {'tokenizer.chat_template': \"\\n\\n<|start_header_id|>system<|end_header_id|>\\n{{ system_message }}<|eot_id|>\\n{% for message in messages %}\\n<|start_header_id|>{{ message['role'] }}<|end_header_id|>\\n{{ message['content'] }}<|eot_id|>\\n{% endfor %}\\n<|start_header_id|>assistant<|end_header_id|>\\n\", 'tokenizer.ggml.add_bos_token': 'true', 'tokenizer.ggml.padding_token_id': '128004', 'tokenizer.ggml.eos_token_id': '128009', 'tokenizer.ggml.add_sep_token': 'false', 'general.type': 'model', 'tokenizer.ggml.bos_token_id': '128000', 'tokenizer.ggml.model': 'gpt2', 'llama.embedding_length': '4096', 'llama.vocab_size': '128256', 'tokenizer.ggml.pre': 'llama-bpe', 'llama.attention.value_length': '128', 'llama.attention.head_count': '32', 'llama.attention.key_length': '128', 'llama.attention.head_count_kv': '8', 'llama.context_length': '131072', 'general.file_type': '15', 'general.finetune': 'Instruct-bnb-4bit-financial-advisor-qlora-GGUF', 'llama.block_count': '32', 'general.size_label': '8B', 'llama.feed_forward_length': '14336', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'general.architecture': 'llama', 'general.basename': 'Meta-Llama-3.1', 'general.quantization_version': '2', 'llama.rope.dimension_count': '128', 'llama.rope.freq_base': '500000.000000', 'general.name': 'Meta Llama 3.1 8B Instruct Bnb 4bit Financial Advisor Qlora GGUF'}\n",
      "Available chat formats from metadata: chat_template.default\n",
      "Using gguf chat template: \n",
      "\n",
      "<|start_header_id|>system<|end_header_id|>\n",
      "{{ system_message }}<|eot_id|>\n",
      "{% for message in messages %}\n",
      "<|start_header_id|>{{ message['role'] }}<|end_header_id|>\n",
      "{{ message['content'] }}<|eot_id|>\n",
      "{% endfor %}\n",
      "<|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "Using chat eos_token: <|eot_id|>\n",
      "Using chat bos_token: <|begin_of_text|>\n"
     ]
    }
   ],
   "source": [
    "llm = Llama(\n",
    "      model_path=\"/Users/jordynojeda/Documents/Fine-Tuned-LLM-with-Retrieval-Augmented-Generation-FT-RAG/models/Meta-Llama-3.1-8B-Instruct-bnb-4bit-financial-advisor-qlora-GGUF/financial_advisor.Q4_K_M.gguf\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7daf549a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 11 prefix-match hit, remaining 13 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1945.53 ms\n",
      "llama_perf_context_print: prompt eval time =     678.78 ms /    13 tokens (   52.21 ms per token,    19.15 tokens per second)\n",
      "llama_perf_context_print:        eval time =    3737.85 ms /    65 runs   (   57.51 ms per token,    17.39 tokens per second)\n",
      "llama_perf_context_print:       total time =    4433.95 ms /    78 tokens\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'id': 'chatcmpl-e0b5bca8-7183-4c7b-8146-219f03f434d7',\n",
       " 'object': 'chat.completion',\n",
       " 'created': 1753188469,\n",
       " 'model': '/Users/jordynojeda/Documents/Fine-Tuned-LLM-with-Retrieval-Augmented-Generation-FT-RAG/models/Meta-Llama-3.1-8B-Instruct-bnb-4bit-financial-advisor-qlora-GGUF/financial_advisor.Q4_K_M.gguf',\n",
       " 'choices': [{'index': 0,\n",
       "   'message': {'role': 'assistant',\n",
       "    'content': \"\\nI can provide you with general information on a wide range of financial questions. However, please note that I'm not a financial advisor and my advice should not be taken as professional advice. If you have specific questions about personal finance, investing, or retirement planning, I'll do my best to provide clear, actionable advice.\"},\n",
       "   'logprobs': None,\n",
       "   'finish_reason': 'stop'}],\n",
       " 'usage': {'prompt_tokens': 24, 'completion_tokens': 65, 'total_tokens': 89}}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.create_chat_completion(\n",
    "      messages = [\n",
    "        {\"role\": \"user\", \"content\": \"How have knowledge of finacial questions?\"},\n",
    "      ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "02a7b2ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chat_template.default\n"
     ]
    }
   ],
   "source": [
    "print(llm.chat_format)  # This might return \"llama-2\", \"mistral\", \"chatml\", or something else."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fine-tuning-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
